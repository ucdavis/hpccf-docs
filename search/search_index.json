{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":""},{"location":"#using-this-documentation","title":"Using This Documentation","text":"<p>Before contacting HPCCF support, first try searching this documentation. This site provides information on accessing and interacting with the cluster, an overview of available software ecosystems, and tutorials for commonly used software and access patterns. It is split into a Users section for end-users and an Admin section with information relevant to system administators. This documentation is being actively expanded as Franklin's software and userbase grows.</p> <p>This site is written in markdown using MkDocs with the Material for MkDocs theme. If you would like to contribute, you may fork our repo  and submit a pull request .</p>"},{"location":"franklin/","title":"Franklin","text":"<p>Franklin is a high performance computing (HPC) cluster for the College of Biological Sciences at UC Davis. Its primary use is for research in genetics, genomics, and proteomics, structural biology via cryogenic electron microscopy, computational neuroscience, and generally, the computional biology workflows related to those fields. Franklin currently consists of 6 AMD  CPU nodes each with 128 physical and 256 logical cores and 1TB of RAM, 5 GPU nodes with a total of 40 Nvidia  RTX A4000, RTX A5000, and RTX 2080 TI GPUs, and a collection of ZFS  file servers providing over 2PB of storage.</p>"},{"location":"franklin/scheduling/","title":"Job Scheduling","text":""},{"location":"franklin/scheduling/#partitions","title":"Partitions","text":""},{"location":"franklin/scheduling/#quality-of-service","title":"Quality of Service","text":""},{"location":"franklin/storage/","title":"Storage","text":""},{"location":"franklin/storage/#home-directories","title":"Home Directories","text":"<p>All users are allocated 20GB of storage for their home directory. This space is free and not associated with lab storage quotas.</p>"},{"location":"franklin/storage/#lab-storage-allocations","title":"Lab Storage Allocations","text":"<p>Research data should be stored on lab storage allocations.  These allocations are mounted at <code>/group/[PI_GROUP_NAME]</code>. N ote that these directories are mounted as-needed, so your particular allocation might not show up when you run <code>ls /group</code>; you will need to access the path directly. You can find your PI group name by running <code>groups</code>: this will output your user name and a name ending in <code>grp</code>. The latter corresponds to the directory name under <code>/group</code>, unless otherwise requested by your PI.</p>"},{"location":"franklin/software/cryoem/","title":"Cryo-EM Software Stack","text":""},{"location":"franklin/software/cryoem/#relion","title":"Relion","text":"<p>Franklin has multiple CPU and GPU optimized versions of the Relion cryo-EM structural determination package. The head node has been configured to support X11 forwarding, enabling the Relion GUI to be launched. Relion jobs are submitted for batch processing on the cluster node via Slurm. Each Relion module exports the necessary configurations to pre-fill job submission and dependency information in the GUI, and we have defined additional GUI fields to further configure Slurm parameters. We are also maintaining an additional software package, <code>relion-helper</code>, to assist users in switching between Relion modules within the same project.</p> <p>Your first step is deciding which Relion variant you should use. We recommend version 4.0.0, as it is the currently-supported stable release. There are three variants of this version: <code>relion/cpu/4.0.0+amd</code>, <code>relion/gpu/4.0.0+amd</code>, and <code>relion/gpu/4.0.0+intel</code>, which correspond to the CPU optimized, GPU with AMD CPU optimized, and GPU with Intel CPU optimized builds, respectively. More information about these modules is available in the Module Variants section. In general, unless you have access to the three GPU nodes owned by the Al-Bassam lab, you can ignore the Intel variants, and use the CPU <code>+amd</code> version for multi-node CPU only jobs and the GPU <code>+amd</code> version if you have access to a GPU node.</p> <p>If you are completely unfamiliar with Relion, you should start with the tutorial.</p> <p>Note</p> <p>Because Relion is GUI driven, you need to <code>ssh</code> to Franklin with X11 forwarding enabled. Instructions for enabling X11 forwarding can be found in the Access section.</p>"},{"location":"franklin/software/cryoem/#launching-relion","title":"Launching Relion","text":"<p>Make sure you have loaded one of the Relion modules:</p> <pre><code>$ module list relion\n\nCurrently Loaded Modules Matching: relion\n  1) relion/gpu/4.0.0+amd   2) relion-helper/0.2\n</code></pre> <p>Change your working directory your Relion project directory and type <code>relion</code>. The Relion GUI should then pop up locally. There will be a bit of latency when using it, especially if you are off campus. You may be able to reduce latency by enabling SSH compression.</p> <p> </p> The relion start screen."},{"location":"franklin/software/cryoem/#dependency-configuration","title":"Dependency Configuration","text":"<p>The paths to software that different Relion jobs use will be automatically filled in. Editing these paths, unless you really, really know what you are doing, is not recommended and will likely result in problems, as some of these dependencies are compiled with architecture-specific flags that match their Relion variant.</p> <p> </p> Pre-filled dependent program path. <p>Danger</p> <p>If you plan to switch between Relion modules within the same project, you must use the procedure described in the relion-helper section. Failure to do so will result in execution errors.</p>"},{"location":"franklin/software/cryoem/#slurm-configuration","title":"Slurm Configuration","text":"<p>Our Relion deployment has additional fields in the Running tabs. These new fields are:</p> <ul> <li>Email: The email to which Slurm will send job status updates. Fills the <code>--mail-user</code> <code>sbatch</code>/<code>srun</code> parameter.</li> <li>Memory per CPU: Fills the Slurm <code>--memory-per-cpu</code> parameter. Total RAM use of a job will be (Number of MPI procs) * (Number of Threads) * (Memory per CPU), when the Number of Threads field is available; otherwise it will be  (Number of MPI procs) * (Memory per CPU).</li> <li>Job Time: Fills Slurm's <code>--time</code> parameter.</li> <li>GPU Resources: Only available in the GPU modules. Number (and optionally type) of GPUs to request for this job. If only an integer is supplied, will request any GPU. If <code>TYPE:NUM</code> is supplied (example: <code>a4000:4</code>), specific models of GPU will be requested. See the Resources section for more information on available GPU types.</li> </ul> CPU BuildGPU Build <p> The <code>relion/cpu</code> modules lack the GPU resources field. Note the submission script as well. </p> <p> The <code>relion/gpu</code> module has an extra field for GPU resources. Also note the differing submission script. </p> <p>The default GUI fields serve their original purposes:</p> <ul> <li>Number of MPI procs: This will fill the Slurm <code>--ntasks</code> parameter. These tasks may be distributed across multiple nodes, depending on the number of Threads requested. For GPU runs, this should be the number of GPUs + 1.</li> <li>Number of Threads: The will fill the Slurm <code>--cpus-per-task</code> parameter, which means it is the number of threads per MPI proc. Some job types do not expose this field, as they can only be run with a single-thread per MPI proc.</li> <li>Queue name: The Slurm partition to submit to, filling the <code>--partition</code> parameter. More information on partitions can be found in the Queueing section.</li> <li>Standard submission script: The location of the Slurm job script template that will be used. This field will be filled with the appropriate template for the loaded Relion module by default, and should not be changed.For advanced users only: if you are familiar with Relion and want to further fine-tune your Slurm scripts, you can write your own based on the provided templates found in <code>/share/apps/spack/templates/hpccf/franklin</code> or in our spack GitHub repo.</li> <li>Minimum dedicated cores per node: Unused on our system.</li> </ul>"},{"location":"franklin/software/cryoem/#switching-between-relion-modules-relion-helper","title":"Switching Between Relion Modules: relion-helper","text":"<p>Sometimes, you may wish to use different Relion modules for different tasks while working within the same project --  perhaps you'd prefer to use the CPU-optimized version for CTF estimation and the GPU-optimized version for 3D refinement. This does not work out of the box. Relion fills the filesystem paths of its dependencies and templates from environment variables, and those environment variables are set in the modulefiles of the differing Relion builds. However, when a Relion job is run, those paths are cached in hidden <code>.star</code> files in the project directory, and the next time Relion is run, it fills those paths from the cache files instead of the environment variables. This means that, after switching modules, the cached location of the previous module will be used, instead of the exported environment variables from the new module. This causes major breakage due to dependencies having different compilation options to match the parent Relion they are attached to and Slurm templates having different configuration options available.</p> <p>Luckily, we have a solution! We wrote and are maintaining relion-helper, a simple utility that updates the cached variables in a project to match whatever Relion module is currently loaded. Let's go over example use of the tool.</p> <p>In this example, assume we have a relion project directory at <code>/path/to/my/project</code>. We ran some steps with the module <code>relion/gpu/4.0.0+amd</code>, and now want to switch to <code>relion/cpu/4.0.0+amd</code>. First, let's swap modules:</p> <pre><code>$ module unload relion/gpu/4.0.0+amd amdfftw/3.2+amd: unloaded.\nctffind/4.1.14+amd: unloaded.\nrelion/gpu/4.0.0+amd: unloaded.\nmotioncor2/1.5.0: unloaded.\ngctf/1.06: unloaded.\nghostscript/9.56.1: unloaded.\n$ module load relion/cpu/4.0.0+amd.lua amdfftw/3.2+amd: loaded.\nctffind/4.1.14+amd: loaded.\nrelion/cpu/4.0.0+amd: loaded.\nmotioncor2/1.5.0: loaded.\ngctf/1.06: loaded.\nghostscript/9.56.1: loaded.\n</code></pre> <p>And load relion-helper:</p> <pre><code>$ module load relion-helper relion-helper/0.2: loaded.\n$ relion-helper -h\nusage: relion-helper [-h] {reset-cache} ...\npositional arguments:\n  {reset-cache}\noptions:\n  -h, --help     show this help message and exit\n</code></pre> <p>Now, change to the project directory:</p> <pre><code>$ cd /path/to/my/project\n</code></pre> <p>Then, run the utility.  It will pull the updated values from the appropriate environment variables that were exported by the new module and write them to the cache files in-place.</p> <pre><code>$ relion-helper reset-cache\n&gt; .gui_ctffindjob.star:41:\n  qsub_extra2: 2 =&gt; 10000\n&gt; .gui_ctffindjob.star:42:\n  qsub_extra3: 10000 =&gt; 12:00:00\n&gt; .gui_ctffindjob.star:43:\n  qsubscript: /share/apps/spack/templates/hpccf/franklin/relion.4.0.0.gpu.zen2.slurm.template.sh =&gt; \n/share/apps/spack/templates/hpccf/franklin/relion.4.0.0.cpu.slurm.template.sh\n&gt; .gui_class2djob.star:53:\n  qsub_extra2: 2 =&gt; 10000\n&gt; .gui_class2djob.star:54:\n  qsub_extra3: 10000 =&gt; 12:00:00\n&gt; .gui_class2djob.star:55:\n  qsubscript: /share/apps/spack/templates/hpccf/franklin/relion.4.0.0.gpu.zen2.slurm.template.sh =&gt; \n/share/apps/spack/templates/hpccf/franklin/relion.4.0.0.cpu.slurm.template.sh\n&gt; .gui_autopickjob.star:63:\n  qsub_extra2: 2 =&gt; 10000\n&gt; .gui_autopickjob.star:64:\n  qsub_extra3: 10000 =&gt; 12:00:00\n&gt; .gui_autopickjob.star:65:\n  qsubscript: /share/apps/spack/templates/hpccf/franklin/relion.4.0.0.gpu.zen2.slurm.template.sh =&gt; \n/share/apps/spack/templates/hpccf/franklin/relion.4.0.0.cpu.slurm.template.sh\n&gt; .gui_importjob.star:38:\n  qsub_extra2: 2 =&gt; 10000\n...\n</code></pre> <p>The above output is truncated for brevity. For each cached variable it updates, it reports the name of the cache file, the line number of the change, and the  variable name and value of the change. You can now launch Relion and continue with your work.</p> <p>Each time you want to switch Relion modules for a project, you will need to run this after loading the new module.</p> <p>For now, relion-helper only has the <code>reset-cache</code> subcommand. You can skip <code>cd</code>ing to the project directory by passing the project directory to it instead:</p> <pre><code>$ relion-helper reset-cache -p /path/to/my/project\n</code></pre> <p>Although the changes are made in-place, it leaves backups of the modified files, in case you are concerned about bugs. The original files are of the form <code>.gui_[JOBNAME].star</code>, and the backups are suffixed with <code>.bak</code>:</p> <pre><code>$ ls -al /path/to/my/project\ntotal 317\ndrwxrwxr-x 10 camw camw   31 Feb  3 10:02 .\ndrwxrwxr-x  4 camw camw    6 Jan 12 12:58 ..\ndrwxrwxr-x  5 camw camw    5 Jan 12 12:46 .Nodes\ndrwxrwxr-x  2 camw camw    2 Jan 12 12:40 .TMP_runfiles\n-rw-rw-r--  1 camw camw 1959 Feb  3 10:02 .gui_autopickjob.star\n-rw-rw-r--  1 camw camw 1957 Feb  3 10:01 .gui_autopickjob.star.bak\n-rw-rw-r--  1 camw camw 1427 Feb  3 10:02 .gui_class2djob.star\n-rw-rw-r--  1 camw camw 1425 Feb  3 10:01 .gui_class2djob.star.bak\n-rw-rw-r--  1 camw camw 1430 Feb  3 10:02 .gui_ctffindjob.star\n-rw-rw-r--  1 camw camw 1428 Feb  3 10:01 .gui_ctffindjob.star.bak\n...\n</code></pre> <p>Warning</p> <p>We do not recommend changing between major Relion versions within the same project: ie, from 3.0.1 to 4.0.0.</p>"},{"location":"franklin/software/cryoem/#module-variants","title":"Module Variants","text":"<p>There are currently six variations of Relion available on Franklin. Versions 3.1.3 and 4.0.0 are available, each with:</p> <ul> <li>A CPU-optimized build compiled for AMD processors: <code>relion/cpu/[VERSION]+amd</code></li> <li>A GPU-optimized build compiled for AMD processors: <code>relion/gpu/[VERSION]+amd</code></li> <li>A GPU-optimized build compiled for Intel processors: <code>relion/gpu/[VERSION]+intel</code></li> </ul> <p>The CPU-optimized builds were configured with <code>-DALTCPU=True</code> and without CUDA support. For Relion CPU jobs, they will be much faster than the GPU variants. The AMD-optimized <code>+amd</code> variants were compiled with <code>-DAMDFFTW=ON</code> and linked against the <code>amdfftw</code> implementation of  <code>FFTW</code>, in addition to having Zen 2 microarchitecture flags specified to GCC. The <code>+intel</code> variants were compiled with AVX2 support and configured with the <code>-DMKLFFT=True</code> flag, so they use the Intel OneAPI MKL implementation of <code>FFTW</code>. All the GPU variants are targeted to a CUDA compute version of 7.5. The full Cryo-EM software stack is defined in the HPCCF spack configuration repository, and we maintain our own Relion spack package definition. More information on the configurations described here can be found in the Relion docs.</p> <p>The different modules may need to be used with different Slurm resource directives, depending on their variants. The necessary directives, given a module and job partition, are as follows:</p> Module Name Slurm Partition Slurm Directives <code>relion/cpu/[3.1.3,4.0.0]+amd</code> <code>low</code> <code>--constraint=amd</code> <code>relion/cpu/[3.1.3,4.0.0]+amd</code> <code>high</code> N/A <code>relion/gpu/[3.1.3,4.0.0]+amd</code> <code>low</code> <code>--constraint=amd --gres=gpu:[$N_GPUs]</code> or <code>--gres=gpu:[a4000,a5000]:[$N_GPUs]</code> <code>relion/gpu/[3.1.3,4.0.0]+amd</code> <code>jalettsgrp-gpu</code> <code>--gres=gpu:[$N_GPUs]</code> <code>relion/gpu/[3.1.3,4.0.0]+amd</code> <code>mmgdept-gpu</code> <code>--gres=gpu:[$N_GPUs]</code> <code>relion/gpu/[3.1.3,4.0.0]+intel</code> <code>low</code> <code>--constraint=intel --gres=gpu:[$N_GPUs]</code> or <code>--gres=gpu:[rtx_2080_ti]:[$N_GPUs]</code> <code>relion/gpu/[3.1.3,4.0.0]+intel</code> <code>jawdatgrp-gpu</code> <code>--gres=gpu:[$N_GPUs]</code> <p>For example, to use the CPU-optimized Relion module <code>relion/cpu/4.0.0+amd</code> on the free, preemptable <code>low</code> partition, you should submit jobs with <code>--constraint=amd</code> so as to eliminate the Intel nodes in that partition from consideration. However, if you have access to and are using the <code>high</code> partition with the same module, no additional Slurm directives are required, as the <code>high</code> partition only has CPU compute nodes. Alternatively, if you were using an AMD-optimized GPU version, like <code>relion/gpu/4.0.0+amd</code>, and wished to use 2 GPUs on the <code>low</code> partition, you would need to provide both the <code>--constraint=amd</code> and a <code>--gres=gpu:2</code> directive, in order to get an AMD node on the partition along with the required GPUs. Those with access to and submitting to the <code>mmgdept-gpu</code> queue would need only to specify <code>--gres=gpu:2</code>, as that partition only has AMD nodes in it.</p> <p>Note</p> <p>If you are submitting jobs via the GUI, these Slurm directives will already be taken care of for you. If you wish to submit jobs manually, you can get the path to Slurm submission template for the currently-loaded module from the <code>$RELION_QSUB_TEMPLATE</code> environment variable; copying this template is a good starting place for building your batch scripts.</p>"},{"location":"franklin/software/cryoem/#ctffind","title":"ctffind","text":"<p>Our installation of CTFFIND4 has <code>+amd</code> and <code>+intel</code> variants which, like Relion, are linked against <code>amdfftw</code> and Intel OneAPI MKL, respectively. The Slurm <code>--constraint</code> flags should be used with these as well, when appropriate, as indicated in the Relion directive table. Each Relion module has its companion CTFFIND4 module as a dependency, so the appropriate version will automatically be loaded when you load Relion, and the proper environment variables are set for the Relion GUI to point at them.</p>"},{"location":"franklin/software/cryoem/#motioncor2","title":"MotionCor2","text":"<p>We have deployed MotionCor2 binaries which have been patched to link against the appropriate version of CUDA. These are targetted at a generic architecture, as the source code is not available. Like CTFFIND4, this module is brought in by Relion and the proper environment variables set for Relion to use it.</p>"},{"location":"franklin/software/cryoem/#gctf","title":"Gctf","text":"<p>Gctf binaries have been patched and deployed in the same manner as MotionCor2.</p>"},{"location":"franklin/software/modules/","title":"Macro Rendering Error","text":"<p>File: <code>franklin/software/modules.md</code></p> <p>FileNotFoundError: [Errno 2] No such file or directory: '../spack-ucdavis/modulefiles/hpccf/franklin/module-index.yaml'</p> <pre><code>Traceback (most recent call last):\n  File \"/opt/hostedtoolcache/Python/3.11.3/x64/lib/python3.11/site-packages/mkdocs_macros/plugin.py\", line 523, in render\n    return md_template.render(**page_variables)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.3/x64/lib/python3.11/site-packages/jinja2/environment.py\", line 1301, in render\n    self.environment.handle_exception()\n  File \"/opt/hostedtoolcache/Python/3.11.3/x64/lib/python3.11/site-packages/jinja2/environment.py\", line 936, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 40, in top-level template code\n  File \"/home/runner/work/hpccf-docs/hpccf-docs/macros.py\", line 75, in spack_modules_list\n    modules = build_module_data(parse_spack_index(path), path)\n                                ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/runner/work/hpccf-docs/hpccf-docs/macros.py\", line 8, in parse_spack_index\n    with open(os.path.join(modulefiles_path, 'module-index.yaml')) as fp:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '../spack-ucdavis/modulefiles/hpccf/franklin/module-index.yaml'\n</code></pre>"},{"location":"general/access/","title":"Accessing the Clusters","text":""},{"location":"general/access/#x11-forwarding","title":"X11 Forwarding","text":"<p>Some software has a Graphical User Interface (GUI), and so requires X11 to be enabled. X11 forwarding allows an application on a remote server (in this case, Franklin) to render its GUI on a local system (your computer). How this is enabled depends on the operating system the computer you are using to access Franklin is running.</p>"},{"location":"general/access/#linux","title":"Linux","text":"<p>If you are SSHing from a Linux distribution, you likely already have an X11 server running locally, and can support forwarding natively. If you are on campus, you can use the <code>-Y</code> flag to enable it, like:</p> <pre><code>$ ssh -Y [USER]@franklin.hpc.ucdavis.edu\n</code></pre> <p>If you are off campus on a slower internet connection, you may get better performance by enabling compression with:</p> <pre><code>$ ssh -Y [USER]@franklin.hpc.ucdavis.edu\n</code></pre>"},{"location":"general/access/#macos","title":"MacOS","text":"<p>MacOS does not come with an X11 implementation out of the box. You will first need to install the free, open-source XQuartz package, after which you can use the same <code>ssh</code> flags as described in the Linux instructions.</p>"},{"location":"general/access/#windows","title":"Windows","text":"<p>If you are using our recommend windows SSH client, MobaXterm, X11 forwarding should be enabled by default. You can confirm this by checking that the <code>X11-Forwarding</code> box is ticked under your Franklin session settings. For off-campus access, you may want to tick the <code>Compression</code> box as well.</p>"},{"location":"scheduler/","title":"Slurm","text":"<p>HPC clusters run job schedulers to distribute and manage computational resources. Generally, schedulers:</p> <ul> <li>Manage and enforce resource constraints, such as execution time, number of CPU cores, and amount of RAM a job may use;</li> <li>Provide tools for efficient communication between nodes during parallel workflows;</li> <li>Fairly coordinate the order and priority of job execution between users;</li> <li>Monitor the status and utilization of nodes.</li> </ul> <p></p> <p>HPCCF clusters use Slurm for job scheduling. A central controller runs on one of the file servers, which users submit jobs to from the access node using the <code>srun</code> and <code>sbatch</code> commands. The controller then determines a priority for the job based on the resources requested and schedules it on the queue. Priority calculation can be complex, but the overall goal of the scheduler is to optimize a tradeoff between throughput on the cluster as a whole and turnaround time on jobs.</p> <p>The Commands section describes how to manage jobs and check cluster status using standard Slurm commands. The Resources section describes how to request computing resources for jobs. The Job Scripts section includes examples of job scripts to be used with <code>sbatch</code>.</p>"},{"location":"scheduler/commands/","title":"Jobs","text":"<p>After logging in to a cluster, your session exists on the head node: a single, less powerful computer that serves as the gatekeeper to the rest of the cluster. To do actual work, you will need to write submission scripts that define your job and submit them to the cluster along with resource requests.</p>"},{"location":"scheduler/commands/#batch-jobs-sbatch","title":"Batch Jobs: <code>sbatch</code>","text":"<p>Most of the time, you will want to submit jobs in the form of job scripts. The batch job script specifies the resources needed for the job, such as the number of nodes,  cores, memory, and walltime. A simple example would be:</p> jobscript.sh<pre><code>#!/bin/bash \n# (1)\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --time=01:00:00\n#SBATCH --mem=100MB\n#SBATCH --partition=low\necho \"Running on $(hostname)\"\n</code></pre> <ol> <li>This will determine the shell Slurm uses to execute your script. You could, for example, use <code>/bin/sh</code> or <code>/bin/zsh</code>.</li> </ol> <p>Which can be submitted to the scheduler by running:</p> <pre><code>$ sbatch jobscript.sh\nSubmitted batch job 629\n</code></pre> <p>The job script is a normal shell script -- note the <code>#!/bin/bash</code> -- that contains additional directives. <code>#SBATCH</code> lines specify directives to be sent to the scheduler; in this case, our resource requests:</p> <ul> <li><code>--ntasks</code>: Number of tasks to run. Slurm may schedule tasks on the same or different nodes.</li> <li><code>--cpus-per-task</code>: Number of CPUs (cores) to allocate per task.</li> <li><code>--time</code>: Maximum wallclock time for the job.</li> <li><code>--mem</code>: Maximum amount of memory for the job.</li> <li><code>--partition</code>: The queue partition to submit to. See the queueing section for more details.</li> </ul> Warning <p>Jobs that exceed their memory or time constraints will be automatically killed. There is no limit on spawning threads, but keep in mind that using far more threads than requested cores will result in rapidly decreasing performance.</p> <p><code>#SBATCH</code> directives directly correspond to arguments passed to the <code>sbatch</code> command. As such, one could remove the lines starting with <code>#SBATCH</code> from the previous job script and submit it with:</p> <pre><code>$ sbatch --ntasks=1 --cpus-per-task=1 --time=01:00:00 --mem=100MB --partition=low jobscript.sh\n</code></pre> <p>Using directives with job scripts is recommended, as it helps you document your resource requests.</p> <p>Try <code>man sbatch</code> or visit the official docs for more options. More information on resource requests can be found in the Resources section, and more examples on writing job scripts can be found in the Job Scripts section.</p>"},{"location":"scheduler/commands/#interactive-jobs-srun","title":"Interactive jobs: <code>srun</code>","text":"<p>Sometimes, you want to run an interactive shell session on a node, such as running an IPython session. <code>srun</code> takes the same parameters as <code>sbatch</code>, while also allowing you to specify a shell. For example:</p> <pre><code>$ srun --ntasks=1 --time=01:00:00 --mem=100MB --partition=low --pty /bin/bash\nsrun: job 630 queued and waiting for resources\nsrun: job 630 has been allocated resources\ncamw@c-8-42:~$\n</code></pre> <p>Note that addition of the <code>--pty /bin/bash</code> argument. You can see that the job is queued and then allocated resources, but instead of exiting, you are brought to a new prompt. In the example above, the user <code>camw</code> has been moved onto the node <code>c-8-42</code>, which is indicated by the new terminal prompt, <code>camw@c-8-42</code>. The same resource and time constraints apply in this session as in <code>sbatch</code> scripts.</p> Note <p>This is the only way to get direct access to a node: you will not be able to simply do <code>ssh c-8-42</code>, for example.</p> <p>Try <code>man srun</code> or visit the official docs for more options.</p>"},{"location":"scheduler/commands/#listing-jobs-squeue","title":"Listing jobs: <code>squeue</code>","text":"<p><code>squeue</code> can be used to monitor running and queued jobs. Running it with no arguments will show all the jobs on the cluster; depending on how many users are active, this could be a lot!</p> <pre><code>$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n               589 jawdatgrp Refine3D adtaheri  R 1-13:51:39      1 gpu-9-18\n               631       low jobscrip     camw  R       0:19      1 c-8-42\n               627       low Class2D/ mashaduz  R      37:11      1 gpu-9-58\n...\n</code></pre> <p>To view only your jobs, you can use <code>squeue --me</code>.</p> <pre><code>$ squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n               631       low jobscrip     camw  R       0:02      1 c-8-42\n</code></pre> <p>The format -- which columns and their width -- can be tuned with the <code>--format</code> parameter. For example, you might way to also include how many cores the job requested, and widen the fields:</p> <pre><code>$ squeue --format=\"%10i %.9P %.20j %.10u %.3t %.25S %.15L %.10C %.6D %.20R\"\nJOBID      PARTITION                 NAME       USER  ST                START_TIME       TIME_LEFT       CPUS  NODES     NODELIST(REASON)\n589        jawdatgrp     Refine3D/job015/   adtaheri   R       2023-01-31T22:51:59         9:58:38          6      1             gpu-9-18\n627              low      Class2D/job424/   mashaduz   R       2023-02-02T12:06:27        11:13:06         60      1             gpu-9-58\n</code></pre> <p>Try <code>man squeue</code> or visit the official docs for more options.</p>"},{"location":"scheduler/commands/#canceling-jobs-scancel","title":"Canceling jobs: <code>scancel</code>","text":"<p>To kill a job before it has completed, use the scancel command:</p> <pre><code>$ scancel JOBID # (1)!\n</code></pre> <ol> <li>Replace <code>JOBID</code> with the ID of your job, which can be obtained with <code>squeue</code>.</li> </ol> <p>You can cancel many jobs at a time; for example, you could cancel all of your running jobs with:</p> <pre><code>$ scancel -u $USER #(1)!\n</code></pre> <ol> <li><code>$USER</code> is an environment variable containing your username, so leave this as is to use it.</li> </ol> <p>Try <code>man scancel</code> or visit the official docs for more options.</p>"},{"location":"scheduler/commands/#job-and-cluster-information-scontrol","title":"Job and Cluster Information:  <code>scontrol</code>","text":"<p><code>scontrol show</code> can be used to display any information known to Slurm. For users, the most useful are the detailed job and node information.</p> <p>To display details for a job, run:</p> <pre><code>$ scontrol show j 635\nJobId=635 JobName=jobscript.sh\n   UserId=camw(1134153) GroupId=camw(1134153) MCS_label=N/A\n   Priority=6563 Nice=0 Account=admin QOS=adminmed\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   RunTime=00:00:24 TimeLimit=01:00:00 TimeMin=N/A\n   SubmitTime=2023-02-02T13:26:24 EligibleTime=2023-02-02T13:26:24\n   AccrueTime=2023-02-02T13:26:24\n   StartTime=2023-02-02T13:26:25 EndTime=2023-02-02T14:26:25 Deadline=N/A\n   PreemptEligibleTime=2023-02-02T13:26:25 PreemptTime=None\n   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2023-02-02T13:26:25 Scheduler=Main\n   Partition=low AllocNode:Sid=nas-8-0:449140\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=c-8-42\n   BatchHost=c-8-42\n   NumNodes=1 NumCPUs=2 NumTasks=1 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=2,mem=100M,node=1,billing=2\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   MinCPUsNode=1 MinMemoryNode=100M MinTmpDiskNode=0\n   Features=(null) DelayBoot=00:00:00\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/home/camw/jobscript.sh\n   WorkDir=/home/camw\n   StdErr=/home/camw/slurm-635.out\n   StdIn=/dev/null\n   StdOut=/home/camw/slurm-635.out\n   Power=\n</code></pre> <p>Where <code>635</code> should be replaced with the ID for your job. For example, you can see that this job was allocated resources on <code>c-8-42</code> (<code>NodeList=c-8-42</code>), that its priority score is 6563 (<code>Priority=6563</code>), and that the script it ran with is located at <code>/home/camw/jobscript.sh</code>.</p> <p>We can also get details on nodes. Let's interrogate <code>c-8-42</code>:</p> <pre><code>$ scontrol show n c-8-42\nNodeName=c-8-42 Arch=x86_64 CoresPerSocket=64 \n   CPUAlloc=4 CPUEfctv=256 CPUTot=256 CPULoad=0.12\n   AvailableFeatures=amd,cpu\n   ActiveFeatures=amd,cpu\n   Gres=(null)\n   NodeAddr=c-8-42 NodeHostName=c-8-42 Version=22.05.6\n   OS=Linux 5.15.0-56-generic #62-Ubuntu SMP Tue Nov 22 19:54:14 UTC 2022 \n   RealMemory=1000000 AllocMem=200 FreeMem=98124 Sockets=2 Boards=1\n   State=MIXED ThreadsPerCore=2 TmpDisk=0 Weight=1 Owner=N/A MCS_label=N/A\n   Partitions=low,high \n   BootTime=2022-12-11T02:25:44 SlurmdStartTime=2022-12-14T10:34:25\n   LastBusyTime=2023-02-02T13:13:22\n   CfgTRES=cpu=256,mem=1000000M,billing=256\n   AllocTRES=cpu=4,mem=200M\n   CapWatts=n/a\n   CurrentWatts=0 AveWatts=0\n   ExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s\n</code></pre> <p><code>CPUAlloc=4</code> tells us that 4 cores are currently allocated on the node. <code>AllocMem=200</code> indicates that 200MiB of RAM are currently allocated, with <code>RealMemory=1000000</code> telling us that there is 1TiB of RAM total on the node.</p>"},{"location":"scheduler/commands/#node-status-sinfo","title":"Node Status: <code>sinfo</code>","text":"<p>Another useful status command is <code>sinfo</code>, which is specialized for displaying information on nodes and partitions. Running it without any arguments gives information on partitions:</p> <pre><code>$ sinfo\nPARTITION     AVAIL  TIMELIMIT  NODES  STATE NODELIST\nlow*             up   12:00:00      3    mix gpu-9-[10,18,58]\nlow*             up   12:00:00      8   idle c-8-[42,50,58,62,70,74],gpu-9-[26,66]\nhigh             up 60-00:00:0      6   idle c-8-[42,50,58,62,70,74]\njawdatgrp-gpu    up   infinite      2    mix gpu-9-[10,18]\njawdatgrp-gpu    up   infinite      1   idle gpu-9-26\n</code></pre> <p>In this case, we can see that there are 3 partially-allocated nodes in the <code>low</code> partition (they have state <code>mix</code>), and that the time limit for jobs on the <code>low</code> partition is 12 hours.</p> <p>Passing the <code>-N</code> flag tells <code>sinfo</code> to display node-centric information:</p> <pre><code>$ sinfo -N\nNODELIST   NODES     PARTITION STATE \nc-8-42         1          low* idle  \nc-8-42         1          high idle  \nc-8-50         1          low* idle  \nc-8-50         1          high idle  \nc-8-58         1          low* idle  \nc-8-58         1          high idle  \nc-8-62         1          low* idle  \nc-8-62         1          high idle  \nc-8-70         1          low* idle  \nc-8-70         1          high idle  \nc-8-74         1          low* idle  \nc-8-74         1          high idle  \ngpu-9-10       1          low* mix   \ngpu-9-10       1 jawdatgrp-gpu mix   \ngpu-9-18       1          low* mix   \ngpu-9-18       1 jawdatgrp-gpu mix   \ngpu-9-26       1          low* idle  \ngpu-9-26       1 jawdatgrp-gpu idle  \ngpu-9-58       1          low* mix   \ngpu-9-66       1          low* idle\n</code></pre> <p>There is an entry for each node in each of its partitions. <code>c-8-42</code> is in both the <code>low</code> and <code>high</code> partitions, while <code>gpu-9-10</code> is in the <code>low</code> and <code>jawdatgrp-gpu</code> partitions.</p> <p>More verbose information can be obtained by also passing the <code>-l</code> or <code>--long</code> flag:</p> <pre><code>$ sinfo -N -l\nThu Feb 02 14:04:48 2023\nNODELIST   NODES     PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON              \nc-8-42         1          low*        idle 256    2:64:2 100000        0      1  amd,cpu none                \nc-8-42         1          high        idle 256    2:64:2 100000        0      1  amd,cpu none                \nc-8-50         1          low*        idle 256    2:64:2 100000        0      1  amd,cpu none                \nc-8-50         1          high        idle 256    2:64:2 100000        0      1  amd,cpu none                \nc-8-58         1          low*        idle 256    2:64:2 100000        0      1  amd,cpu none\n...\n</code></pre> <p>This view gives the nodes' socket, core, and thread configurations, their RAM, and the feature list, which you can read about in the Resources section. Try <code>man scontrol</code> or <code>man sinfo</code>, or visit the official docs for <code>scontrol</code> and <code>sinfo</code>, for more options.</p>"},{"location":"scheduler/resources/","title":"Requesting Resources","text":""},{"location":"scheduler/resources/#resource-types","title":"Resource Types","text":""},{"location":"scheduler/resources/#cpus-cores","title":"CPUs / cores","text":"<p>CPUs are the central compute power behind your jobs. Most scientific software supports multiprocessing (multiple instances of an executable with discrete memory resources, possibly but not necessarily communicating with each other), multithreading (multiple paths, or threads, of execution within a process on a node, sharing the same memory resources, but able to execute on different cores), or both. This allows computation to scale with increased numbers of CPUs, allowing bigger datasets to be analyzed.</p> <p>Slurm's CPU management methods are complex and can quickly become confusing. For the purposes of this documentation, we will provide a simplified explanation; those with advanced needs should consult the Slurm documentation.</p> <p>Slurm follows a distinction between its physically resources -- cluster nodes and CPUs or cores on a node -- and virtual resources, or tasks, which specificy how requested physical resources will be grouped and distributed. By default, Slurm will minimize the number of nodes allocated to a job, and attempt to keep the job's CPU requests localized within a node. Tasks group together CPUs (or other resources): CPUs within a task will be kept together on the same node. Different tasks may end up on different nodes, but Slurm will exhaust the CPUs on a given node before splitting tasks between nodes unless specifically requested.</p> <p>A Complication: SMT / Hyperthreading</p> <p>Slurm understands the distinction between physical and logical cores. Most modern CPUs support Simultaneous Multithreading (SMT), which allows multiple independent processes to run on a single physical core. Although each of these is not a full fledged core, they have independent hardware for certain operations, and can greatly improve scalability for some tasks. However, using an individual thread within a single core makes little sense, as it shares hardware with the other SMT threads on its core; so, Slurm will always keep these threads together. In practice, this means if you ask for an odd number of CPUs, your request will be rounded up so as not to split an SMT thread between different job allocations.</p> <p>The primary parameters controlling these are:</p> <ul> <li><code>--cpus-per-task/-c</code>: How many CPUs to request per task. The number of CPUs requested here will always be on the same node. By default, 1.</li> <li><code>--ntasks/-n</code>: The number of tasks to request. By default, 1.</li> <li><code>--nodes/-N</code>: The minimum number of nodes to request, by default, 1.</li> </ul> <p>Let's explore some examples. The simple request would be to ask for 2 CPUs. We will use <code>srun</code> to request resources and then immediately run the <code>nproc</code> command within the allocation to report how many CPUs are available:</p> <pre><code>$ srun -c 2 nproc srun: job 682 queued and waiting for resources\nsrun: job 682 has been allocated resources\n2\n</code></pre> <p>We asked for 2 CPUs per task, and Slurm gave us 2 CPUs and 1 task. What happens if we ask for 2 tasks instead of 2 CPUs?</p> <pre><code>$ srun -n 2 nproc\nsrun: job 683 queued and waiting for resources\nsrun: job 683 has been allocated resources\n1\n1\n</code></pre> <p>This time, we were given 2 separate tasks, each of which got 1 CPU. Each task ran its own instance of the <code>nproc</code> command, and so each reported <code>1</code>. If we ask for more CPUs per task:</p> <pre><code>$ srun -n 2 -c 2 nproc\nsrun: job 684 queued and waiting for resources\nsrun: job 684 has been allocated resources\n2\n2\n</code></pre> <p>We still asked for 2 tasks, but this time we requested 2 CPUs in each. So, we got 2 instances of <code>nproc</code>, each reported <code>2</code> CPUs in their task.</p> <p>Summary</p> <p>If you want to run multithreaded jobs, use <code>--cpus-per-task N_THREADS</code> and <code>-ntasks 1</code>. If you want a multiprocess job (or an MPI job), increase <code>-ntasks</code>.</p> The SMT Edge Case <p>If we use <code>-c 1</code> without specifying the number of tasks, we might be taken by surprise:</p> <pre><code>$ srun -c 1 nproc     srun: job 685 queued and waiting for resources\nsrun: job 685 has been allocated resources\n1\n1\n</code></pre> <p>We only asked for 1 CPU per task, but we got 2 tasks! This is due to SMT, described in the note above. Because Slurm will not split SMT threads, and there are 2 SMT threads per physical core, the request was rounded up to 2 CPUs total. In order to keep with the 1 CPU-per-task constraint, it spawned 2 tasks. Similarly, if we specify that we only want 1 task, CPUs per task will instead be bumped:</p> <pre><code>$ srun -c 1 -n 1 nproc\nsrun: job 686 queued and waiting for resources\nsrun: job 686 has been allocated resources\n2\n</code></pre>"},{"location":"scheduler/resources/#nodes","title":"Nodes","text":"<p>Let's explore multiple nodes a bit further. We have seen previously that the <code>-n/ntasks</code> parameter will allocate discrete groups of cores. In our prior examples, however, we used small resource requests. What happens when we want to distribute jobs across nodes?</p> <p>Slurm uses the block distribution method by default to distribute tasks betwee nodes. It will exhaust all the CPUs on a node with task groups before moving to a new node. For these examples, we're going to create a script that reports both the hostname (ie, the node) and the number of CPUs:</p> host-nprocs.sh<pre><code>#!/bin/bash\necho `hostname`: `nproc`\n</code></pre> <p>And make it executable with <code>chmod +x host-nprocs.sh</code>. </p> <p>Now let's make a multiple-task request:</p> <pre><code>$ srun -c 2 -n 2 ./host-nprocs.sh\nsrun: job 691 queued and waiting for resources\nsrun: job 691 has been allocated resources\nc-8-42: 2\nc-8-42: 2\n</code></pre> <p>As before, we asked for 2 tasks and 2 CPUs per task. Both tasks were assigned to <code>c-8-42</code>, because it had enough CPUs to fulfill the request. What if it did not?</p> <pre><code>$ srun -c 120 -n 3 ./host-nprocs.sh\nsrun: job 692 queued and waiting for resources\nsrun: job 692 has been allocated resources\nc-8-42: 120\nc-8-42: 120\nc-8-50: 120\n</code></pre> <p>This time, we asked for 3 tasks each with 120 CPUs. The first two tasks were able to be fulfilled by the node <code>c-8-42</code>, but that node did not have enough CPUs to allocate another 120 on top of that. So, the third task was distributed to <code>c-8-50</code>. Thus, this task spanned multiple nodes.</p> <p>Sometimes, we want to make sure each task has its own node. We can achieve this with the <code>--nodes/-N</code> parameter. This specifies the minimum number of nodes the tasks should be allocated across. If we rerun the above example:</p> <pre><code>$ srun -c 120 -n 3 -N 3 ./host-nprocs.sh\nsrun: job 693 queued and waiting for resources\nsrun: job 693 has been allocated resources\nc-8-42: 120\nc-8-50: 120\nc-8-58: 120\n</code></pre> <p>We still asked for 3 tasks and 3 CPUs per task, but this time we specified we wanted a minimum of 3 nodes. As a result, we were allocated portions of <code>c-8-42</code>, <code>c-8-50</code>, and <code>c-8-58</code>.</p>"},{"location":"scheduler/resources/#ram-memory","title":"RAM / Memory","text":"<p>Random Access Memory (RAM) is the fast, volatile storage that your programs use to store data during execution. This can be contrasted with disk storage, which is non-volatile and many orders of magnitude slower to access, and is used for long term data -- say, your sequencing runs or cryo-EM images. RAM is a limited resource on each node, so Slurm enforces memory limits for jobs using cgroups. If a job step consumes more RAM than requested, the step will be killed.</p> <p>Some (mutually exclusive) parameters for requesting RAM are:</p> <ul> <li><code>--mem</code>: The memory required per-node. Usually, you want to use <code>--mem-per-cpu</code>.</li> <li><code>--mem-per-cpu</code>: The memory required per CPU or core. If you requested \\(N\\) tasks, \\(C\\) CPUs per task, and \\(M\\) memory per CPU, your total memory usage will be \\(N * C * M\\). Note that, if \\(N \\gt 1\\), you will have \\(N\\) discrete \\(C * M\\)-sized chunks of RAM requested, possibly across different nodes.</li> <li><code>--mem-per-gpu</code>: Memory required per GPU, which will scale with GPUs in the same way as <code>--mem-per-cpu</code> will with CPUs.</li> </ul> <p>For all memory requests, units can be specified explicitly with the suffixes <code>[K|M|G|T]</code> for <code>[kilobytes|megabytes|gigabytes|terabytes]</code>, with the default units being <code>M</code>/<code>megabytes</code>. So, <code>--mem-per-cpu=500</code> will requested 500 megabytes of RAM per CPU, and <code>--mem-per-cpu=32G</code> will request 32 gigabytes of RAM per CPU.</p> <p>Here is an example of a task overrunning its memory allocation. We will use the <code>stress-ng</code> program to allocate 8 gigabytes of RAM in a job that only requested 200 megabytes.</p> <pre><code>$ srun -n 1 --cpus-per-task 2 --mem-per-cpu 200M stress-ng -m 1 --vm-bytes 8G --oomable         1 \u21b5\nsrun: job 706 queued and waiting for resources\nsrun: job 706 has been allocated resources\nstress-ng: info:  [3037475] defaulting to a 86400 second (1 day, 0.00 secs) run per stressor\nstress-ng: info:  [3037475] dispatching hogs: 1 vm\nstress-ng: info:  [3037475] successful run completed in 2.23s\nslurmstepd: error: Detected 1 oom-kill event(s) in StepId=706.0. Some of your processes may have been killed by the cgroup out-of-memory handler.\nsrun: error: c-8-42: task 0: Out Of Memory\nsrun: launch/slurm: _step_signal: Terminating StepId=706.0\n</code></pre>"},{"location":"scheduler/resources/#gpus-gres","title":"GPUs / GRES","text":""},{"location":"software/conda/","title":"Python and Conda","text":""},{"location":"software/developing/","title":"Developing Software on Franklin","text":""},{"location":"software/modules/","title":"Module System","text":""},{"location":"software/modules/#intro","title":"Intro","text":"<p>High performance compute clusters usually have a variety of software with sometimes conflicting dependencies. Software packages may need to make modifications to the user environment, or the same software may be compiled multiple times to run efficiently on differing hardware within the cluster. To support these use cases, software is managed with a module system that prepares the user environment to access specific software on load and returns the environment to its former state when unloaded. A module is the bit of code that enacts and tracks these changes to the user environment, and the module system is software that runs these modules and the collection of modules it is aware of. Most often, a module is associated with a specific software package at a specific version, but they can also be used to make more general changes to a user environment; for example, a module could load a set of configurations for the BASH shell that set color themes.</p> <p>The two most commonly deployed module systems are environment modules (or <code>envmod</code>) and lmod. Franklin currently uses <code>lmod</code>, which is cross-compatible with <code>envmod</code>.</p>"},{"location":"software/modules/#usage","title":"Usage","text":"<p>The <code>module</code> command is the entry point for users to manage modules in their environment. All module operations will be of the form <code>module [SUBCOMMAND]</code>. Usage information is available on the cluster by running <code>module --help</code>.</p> <p>The basic commands are: <code>module load [MODULENAME]</code> to load a module into your environment; <code>module unload [MODULENAME]</code> to remove that module; <code>module avail</code> to see modules available for loading; and <code>module list</code> to see which modules are currently loaded. We will go over these commands, and some additional commands, in the following sections.</p>"},{"location":"software/modules/#listing","title":"Listing","text":""},{"location":"software/modules/#module-avail","title":"<code>module avail</code>","text":"<p>Lists the modules currently available to load on the system. Some example output would be:</p> <pre><code>$ module avail\n\n----------------------- /share/apps/franklin/modulefiles ------------------------\n   conda/base/4.X                conda/cryolo/1.8.4-cuda-11 (D)    testmod/1.0\n   conda/cryolo/1.8.4-cuda-10    conda/rockstar/0.1\n---------------------- /share/apps/spack/modulefiles/Core -----------------------\n...\n   gatk/3.8.1                         pmix/4.1.2             (L)\n   gatk/4.2.6.1              (D)      prokka/1.14.6\n   gcc/4.9.4                 (L)      raxml-ng/1.0.2\n   gcc/5.5.0                          ray/2.3.1\n   gcc/7.5.0                 (D)      recon/1.05\n   gctf/1.06                 (L)      relion-helper/0.1\n   genrich/0.6                        relion-helper/0.2      (L,D)\n...\n</code></pre> <p>Each entry corresponds to software available for load. Different sections will appear depending on loaded prerequisites, which you can read about under <code>module spider</code>. Where there are multiple versions or variants of a module, a <code>(D)</code> will be listed next to the name of the default version. An <code>(L)</code> indicates that module is currently loaded.</p> <p>Note that this does not necessarily list every possible module on the system. Some software is compiled with a specific compiler and compiler version, and the relevant compiler module must be loaded first. To see all possible modules, use the <code>module spider</code> command.</p>"},{"location":"software/modules/#module-spider","title":"<code>module spider</code>","text":"<p>Lists all possible modules that could be loaded. Some modules require a specific compiler version to be loaded, and these modules will not be listed in <code>module avail</code> unless that module is loaded. Module spider will list these modules anyway. If you run the command with a specific module, it will list the prerequisite modules required to make said module available. For example, if we try to run:</p> <pre><code>$ module load megahit\n\nLmod has detected the following error:  These module(s) or extension(s) exist but cannot be loaded as requested: \"megahit\"\n   Try: \"module spider megahit\" to see how to load the module(s).\n</code></pre> <p>...the load fails. If we then use <code>spider</code>:</p> <pre><code>$ module spider megahit\n\n-----------------------------------------------------------------------------\n  megahit: megahit/1.1.4\n-----------------------------------------------------------------------------\n    You will need to load all module(s) on any one of the lines below before\n    the \"megahit/1.1.4\" module is available to load.\n      gcc/4.9.4\n    Help:\n      MEGAHIT: An ultra-fast single-node solution for large and complex\n      metagenomics assembly via succinct de Bruijn graph\n</code></pre> <p>...we see that we have to first load <code>gcc/4.9.4</code>. Let's do that:</p> <pre><code>$ module load gcc/4.9.4\ngcc/4.9.4: loaded.\n$ module avail\n\n-------------------- /share/apps/spack/modulefiles/gcc/4.9.4 --------------------\n   megahit/1.1.4\n----------------------- /share/apps/franklin/modulefiles ------------------------\n   conda/base/4.X                conda/cryolo/1.8.4-cuda-11 (D)\n   conda/cryolo/1.8.4-cuda-10    conda/rockstar/0.1\n---------------------- /share/apps/spack/modulefiles/Core -----------------------\n   StdEnv                    (L)    libevent/2.1.12        (L)\n   abyss/2.3.1                      mash/2.3\n</code></pre> <p>We are now presented with a new section for those modules that require <code>gcc/4.9.4</code>, with the <code>megahit</code> module listed there. Now, we can load it:</p> <pre><code>$ module load megahit\n\nmegahit/1.1.4: loaded.\n</code></pre> <p>Running <code>module spider</code> without any arguments will list all the modules that could be loaded, unlike <code>module avail</code>, which will list only the modules available for load given any currently-loaded prerequisites.</p>"},{"location":"software/modules/#module-list","title":"<code>module list</code>","text":"<p>Lists the modules currently loaded in the user environment. By default, the output should be similar to:</p> <pre><code>$ module list\n\nCurrently Loaded Modules:\n  1) cuda/11.8.0   3) libevent/2.1.12   5) slurm/22-05-6-1   7) openmpi/4.1.4\n  2) hwloc/2.8.0   4) pmix/4.1.2        6) ucx/1.13.1\n</code></pre> <p>Additional modules will be added or removed as you load and unload them.</p>"},{"location":"software/modules/#module-overview","title":"<code>module overview</code>","text":"<p>This will give a condensed overview of available modules. It is most useful when the list of modules is very large, and there are multiple versions of each module available. The output is a list of each module name followed by the number of versions of the module.</p> <pre><code>$ module overview\n\n----------------------- /share/apps/franklin/modulefiles ------------------------\nconda/base (1)   conda/cryolo (2)   conda/rockstar (1)   testmod (1)\n---------------------- /share/apps/spack/modulefiles/Core -----------------------\nStdEnv         (1)   hmmer            (1)   ncbi-toolkit  (1)\nabyss          (1)   homer            (1)   ncbi-vdb      (1)\namdfftw        (1)   hwloc            (1)   nextflow      (1)\naragorn        (1)   igv              (1)   openmpi       (1)\nbedtools2      (1)   infernal         (1)   orthofinder   (1)\nblast-plus     (1)   intel-oneapi-mkl (1)   orthomcl      (1)\nblast2go       (1)   interproscan     (1)   parallel      (1)\nblat           (1)   iq-tree          (1)   patchelf      (1)\n</code></pre>"},{"location":"software/modules/#loading-and-unloading","title":"Loading and Unloading","text":""},{"location":"software/modules/#module-load","title":"<code>module load</code>","text":"<p>This loads the requested module into the active environment. Loading a module can edit environment variables, such as prepending directories to <code>$PATH</code> so that the executables within can be run, set and unset new or existing environment variables, define shell functions, and generally, modify your user environment arbitrarily. The modifications it makes are tracked, so that when the module is eventually unloaded, any changes can be returned to their former state.</p> <p>Let's load a module.</p> <pre><code>$ module load bwa/0.7.17\nbwa/0.7.17: loaded.\n</code></pre> <p>Now, you have access to the <code>bwa</code> executable. If you try to run <code>bwa mem</code>, you'll get its help output. This also sets the appopriate variables so that you can now run <code>man bwa</code> to view its manpage.</p> <p>Note that some modules have multiple versions. Running <code>module load [MODULENAME]</code> without specifying a version will load the latest version, unless a default has been specified. When there are multiple versions, a <code>(D)</code> will be printed next to the default version when using <code>module avail</code>.</p> <p>Some modules are nested under a deeper hierarchy. For example, <code>relion</code> has six variants, two under <code>relion/cpu</code> and four under <code>relion/gpu</code>. To load these, you must specify the second layer of hierarchy: <code>module load relion</code> will fail, but <code>module load relion/cpu</code> will load the default module under <code>relion/cpu</code>, which has the full name <code>relion/cpu/4.0.0+amd</code>. More information on this system can be found under Organization.</p> <p>The modules on Franklin are all configured to set a <code>$NAME_ROOT</code> variable that points to the installation prefix. This will correspond to the name of the module, minus the version. For example:</p> <pre><code>$  ls -R $BWA_ROOT\n/share/apps/spack/spack-v0.19/opt/spack/linux-ubuntu22.04-x86_64_v3/gcc-9.5.0/bwa-0.7.17-3ogkbh2ixha52dxps2letankhc2dbeax:\nbin  doc  man\n...\n</code></pre> <p>Usually, this will be a very long pathname, as most software on the cluster is managed via the spack build system. This would be most useful if you're developing software on the cluster.</p>"},{"location":"software/rlang/","title":"R and RStudio","text":""}]}